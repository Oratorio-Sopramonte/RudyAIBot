# ðŸ§ª Test Suite for RudyAIBot

This directory contains the comprehensive testing strategy for the RAG pipeline, covering integration tests, and RAG-specific evaluation metrics.

## ðŸ“‚ Directory Structure

```text
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py          # Shared fixtures (Mocks for Qdrant, Gemini, etc.)
â”œâ”€â”€ integration/         # Integration tests
â”‚   â””â”€â”€ test_bot_flow.py   # End-to-end simulated user interactions
â””â”€â”€ evaluation/          # RAG Quality Evaluation
    â”œâ”€â”€ evaluate_rag.py    # Script to calculate Hit Rate & Response Similarity
    â””â”€â”€ test_data/
        â””â”€â”€ golden_dataset.json  # Ground truth Q&A pairs
```

## ðŸš€ How to Run Tests

### 1. Install Development Dependencies
Ensure you have the test packages installed:
```bash
pip install -r requirements.txt
```

### 2. Run Integration Tests
Use `pytest` to run integration tests. 
```bash
pytest tests/integration/ # Run only integration tests
```

### 3. Run RAG Evaluation
To evaluate the RAG pipeline's accuracy against the Golden Dataset, run the evaluation script as a module from the project root. This effectively calculates **Hit Rate** and **Cosine Similarity**.

**Command:**
```bash
python -m tests.evaluation.evaluate_rag
```
